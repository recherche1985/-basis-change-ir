import json
import numpy as np
import matplotlib.pyplot as plt

# Path to results file generated by run.py
RESULTS_FILE = "run.json"

# Load results
with open(RESULTS_FILE, "r", encoding="utf-8") as f:
    results = json.load(f)

# Extract scores safely
bm25_scores = np.array([r.get("BM25", 0) for r in results])
bc_scores = np.array([r.get("BasisChange", 0) for r in results])
query_ids = [r.get("query_id", idx) for idx, r in enumerate(results)]

# Compute per-query gain
gains = bc_scores - bm25_scores

# ===========================
# Plot 1: Gain distribution
# ===========================
plt.figure(figsize=(8,5))
plt.hist(gains, bins=30, color="skyblue", edgecolor="black")
plt.title("Distribution of Basis Change gains over BM25")
plt.xlabel("Gain (Basis Change - BM25)")
plt.ylabel("Number of queries")
plt.grid(True, linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()

# ===========================
# Plot 2: Feedback rank sensitivity
# ===========================
# Here we simulate rank sensitivity by assuming results contain multiple feedback ranks per query
# For simplicity, assume results have a field: "feedback_size_scores": {1: score1, 3: score3, 5: score5}

# Collect feedback sizes
feedback_sizes = sorted(list(results[0].get("feedback_size_scores", {}).keys())) if results else []
if feedback_sizes:
    avg_scores = {size: [] for size in feedback_sizes}
    for r in results:
        for size in feedback_sizes:
            avg_scores[size].append(r["feedback_size_scores"].get(size, np.nan))
    
    # Compute mean per feedback size
    mean_scores = {size: np.nanmean(avg_scores[size]) for size in feedback_sizes}
    
    plt.figure(figsize=(8,5))
    plt.plot(list(mean_scores.keys()), list(mean_scores.values()), marker='o', color='orange')
    plt.title("Basis Change Retrieval Score vs Feedback Size")
    plt.xlabel("Number of Feedback Documents")
    plt.ylabel("Mean Score")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()
else:
    print("No feedback_size_scores found in results; skipping feedback sensitivity plot.")

# ===========================
# Optional: print summary
# ===========================
print("Summary statistics:")
print(f"BM25: mean={bm25_scores.mean():.4f}, std={bm25_scores.std():.4f}")
print(f"Basis Change: mean={bc_scores.mean():.4f}, std={bc_scores.std():.4f}")
print(f"Mean gain: {gains.mean():.4f}, Max gain: {gains.max():.4f}, Min gain: {gains.min():.4f}")
